{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses `pisa<=1.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bank of Functions\n",
    "\n",
    "def haversine_vectorize(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    newlon = lon2 - lon1\n",
    "    newlat = lat2 - lat1\n",
    "    haver_formula = np.sin(newlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(newlon/2.0)**2\n",
    "    dist = 2 * np.arcsin(np.sqrt(haver_formula ))\n",
    "    km = 6367 * dist #6367 for distance in KM for miles use 3958\n",
    "    return round(km,2)\n",
    "\n",
    "def get_haversine(df):\n",
    "    n1 = (df['start_node'].y, df['start_node'].x) # (lat, lon)\n",
    "    n2 = (df['end_node'].y, df['end_node'].x)\n",
    "    dist = haversine(n1, n2)\n",
    "    return dist\n",
    "                 \n",
    "def get_corrected_distance(x):\n",
    "    if(x['euclidean_distance'])<=1:\n",
    "        return x['euclidean_distance']\n",
    "    else:\n",
    "        if (x['road_dist_pop']>=x['euclidean_distance']):\n",
    "            return x['euclidean_distance']\n",
    "        else:\n",
    "            return x['travel_distance']\n",
    "        \n",
    "def getvariables(n, m, X, Y):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    Xvalues = np.zeros(m)\n",
    "    Yvalues = np.zeros(n+m)\n",
    "    for i in range(m):\n",
    "        Xvalues[i]=X[i].x\n",
    "    for i in range(n):\n",
    "        Yvalues[i]=Y[i].x\n",
    "    \n",
    "    return(Xvalues, Yvalues)\n",
    "\n",
    "def OptimizationModel(array_household, current_hospitals_ID, new_hospitals_ID, \n",
    "                      distance_matrix, S, hosp_count, mipGap=.001, trace=False):\n",
    "    \n",
    "    import time\n",
    "    import gurobipy as gb\n",
    "    from gurobipy import GRB\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #Data preprocessing\n",
    "    tstart = time.time()\n",
    "    obj_val_array = []\n",
    "    existinghosp = len(current_hospitals_ID)\n",
    "    m = len(current_hospitals_ID) + len(new_hospitals_ID)\n",
    "    n = len(array_household)\n",
    "    p = existinghosp + 0       #total number of hospitals to be optimized\n",
    "    \n",
    "    # Only keep the combinations of houses/hospitals which are less or equal to the maximum distance\n",
    "    dist = distance_matrix[distance_matrix.distance<=S]\n",
    "    \n",
    "    # collect the indices of the distances below the threshold \n",
    "    II = dist['pop_id']\n",
    "    JJ = dist['fac_id']\n",
    "    \n",
    "    IJ = { i : set() for i in range(n) }\n",
    "    for i,j in zip(II,JJ):\n",
    "        IJ[i].add(j) \n",
    "    \n",
    "    # Create the model\n",
    "    M = gb.Model(\"Facility location problem\")\n",
    "    \n",
    "    M.Params.OutputFlag = trace \n",
    "    M.Params.mipgap     = mipGap\n",
    "    \n",
    "    # Decision variables\n",
    "    X = M.addVars(m, vtype=GRB.BINARY)\n",
    "    Y = M.addVars(n, vtype=GRB.BINARY)\n",
    "    \n",
    "    # Objective\n",
    "    obj = gb.LinExpr( array_household, Y.select('*') )\n",
    "    M.setObjective(obj, gb.GRB.MAXIMIZE)\n",
    "    \n",
    "    # Constraints\n",
    "    # Set existing hospitals to one\n",
    "    M.addConstrs(X[j] == 1 for j in current_hospitals_ID)\n",
    "\n",
    "    # Limit number of hospitals a household is connected to, let a household only connect to an opened facility\n",
    "    M.addConstrs((Y[i] <= (gb.quicksum(X[j] for j in IJ[i]))) for i in range(n))\n",
    "#     M.addConstrs(Y[i] <= (gb.quicksum(X[j] for j in dist['HospCluster'].loc[dist['Pop_ID']==i])) for i in range(n))\n",
    "\n",
    "    \n",
    "    # Limit number of facilities located \n",
    "    s = M.addLConstr(gb.quicksum(X[j] for j in range(m))<= p)\n",
    "    \n",
    "    modelling_time = time.time()-tstart\n",
    "    tstart = time.time()\n",
    "    \n",
    "    # Optimize the model and extract solution\n",
    "    M.optimize() \n",
    "    obj_val = M.objVal\n",
    "    Xvalues, Yvalues = getvariables(n, m, X, Y)\n",
    "\n",
    "    obj_val_array.append([S,0,obj_val,list(Xvalues),list(Yvalues)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Iterate for multiple additional hospital facilities\n",
    "    for each_hosp_count in hosp_count:\n",
    "        M.remove(s)\n",
    "        p = existinghosp + each_hosp_count\n",
    "        s = M.addConstr(gb.quicksum(X[j] for j in range(m))<= p, name = \"Budget\")\n",
    "        \n",
    "        M.optimize()\n",
    "        obj_val = M.objVal\n",
    "        Xvalues, Yvalues = getvariables(n, m, X, Y)\n",
    "\n",
    "        obj_val_array.append([S,each_hosp_count,obj_val,list(Xvalues),list(Yvalues)])\n",
    "    \n",
    "    solving_time = time.time() - tstart \n",
    "    \n",
    "    df_opt_array = pd.DataFrame(obj_val_array)\n",
    "    df_opt_array.columns = ['km','number_of_new_hospitals','count','array_hosp','array_hh']\n",
    "    df_opt_array['number_of_hospitals'] = df_opt_array['number_of_new_hospitals']+existinghosp\n",
    "    df_opt_array['%'] = df_opt_array['count']*100/sum(array_household)\n",
    "    df_opt_array['%'] = df_opt_array['%'].round(1)\n",
    "    \n",
    "    return df_opt_array, modelling_time, solving_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Added by Joaquim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractOptimizationDataFromTravelDistanceMatrix( travel_dist, distance_threshold, col_distance='distance', col_facility_id='fac_id', col_pop_id='pop_id' ):\n",
    "    return travel_dist[travel_dist[col_distance] <= distance_threshold].pivot_table(index=col_facility_id, values=col_pop_id, aggfunc=lambda x : list(set(x)))[col_pop_id].to_dict()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "import folium \n",
    "from folium import features\n",
    "\n",
    "from shapely.geometry import Point, LineString, shape\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "\n",
    "from pyproj import Transformer\n",
    "from shapely.ops import transform\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import bisect\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show    \n",
    "import rioxarray as rxr\n",
    "\n",
    "from rasterstats import zonal_stats\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import HTML as html_print\n",
    "\n",
    "from haversine import haversine_vector, haversine\n",
    "\n",
    "import pandana\n",
    "\n",
    "import pyproj\n",
    "\n",
    "from gurobipy import Model, GRB\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Added by Joaquim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3a2a2284a428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../optimization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmaxcovering\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/Public-Infrastructure-Location-Optimiser/optimization/maxcovering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# helper functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mall_in\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlist_of_lists\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \"\"\"\n\u001b[1;32m     82\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0munique\u001b[0m \u001b[0melements\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../optimization')\n",
    "\n",
    "import maxcovering as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "road_asset_gdf = gpd.read_file('data/road_estrada_new.geojson')\n",
    "road_asset_gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "gdf = road_asset_gdf.to_crs(epsg=3844)\n",
    "gdf['length_code'] = gdf['geometry'].length\n",
    "gdf['length_code'] = gdf['length_code'].round()\n",
    "gdf['length_code'] = gdf['length_code']/1000\n",
    "\n",
    "road_asset_gdf['length_km'] = gdf['length_code']\n",
    "\n",
    "print('Estrada Road Network Data : ', round(road_asset_gdf['length_km'].sum()), 'km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_km = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_network = road_asset_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "road_network['start_node'] = road_network['geometry'].apply(lambda x: Point(x.xy[0][0], x.xy[1][0]) if isinstance(x, LineString) else None)\n",
    "road_network['end_node'] = road_network['geometry'].apply(lambda x: Point(x.xy[0][-1], x.xy[1][-1]) if isinstance(x, LineString) else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "node_list = []\n",
    "for each_val in road_network['start_node'].unique():\n",
    "    node_list.append(each_val)\n",
    "for each_val in road_network['end_node'].unique():\n",
    "    node_list.append(each_val)\n",
    "\n",
    "df_nodes = gpd.GeoDataFrame(node_list,columns=['geometry'])\n",
    "df_nodes = gpd.GeoDataFrame(list(df_nodes['geometry'].unique()),columns=['geometry'])\n",
    "df_nodes['x'] = df_nodes['geometry'].x\n",
    "df_nodes['y'] = df_nodes['geometry'].y\n",
    "\n",
    "df_edge_subset = df_nodes.reset_index()[['index','geometry']]\n",
    "df_edge_subset.columns = ['nodeID','node_geometry']\n",
    "\n",
    "df_edges = pd.merge(road_network,df_edge_subset,left_on='start_node',right_on='node_geometry')\n",
    "df_edges = pd.merge(df_edges,df_edge_subset,left_on='end_node',right_on='node_geometry')\n",
    "\n",
    "df_dist = df_edges[['start_node','end_node']].drop_duplicates()\n",
    "df_dist['len_km'] = df_dist.apply(get_haversine,axis=1)\n",
    "\n",
    "df_edges = pd.merge(df_edges,df_dist,on=['start_node','end_node'])\n",
    "\n",
    "network = pandana.Network(df_nodes['x'], df_nodes['y'], \n",
    "                          df_edges['nodeID_x'], df_edges['nodeID_y'], df_edges[['len_km']],twoway=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.read_csv('data/ppp_TLS_2020_1km_Aggregated_UNadj.csv')\n",
    "population.columns = ['lon','lat','population']\n",
    "pop_gdf = gpd.GeoDataFrame(population, geometry=gpd.points_from_xy(population.lon, population.lat))\n",
    "pop_gdf = pop_gdf[pop_gdf['population']>0]\n",
    "pop_gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "print('Population:', round(pop_gdf['population'].sum()/1000000,2), 'million')\n",
    "\n",
    "pop_gdf['pop_color'] = pd.qcut(pop_gdf['population'],4, labels=['#ffffcc', '#a1dab4', '#41b6c4', '#225ea8'])\n",
    "\n",
    "pop_gdf = pop_gdf.reset_index()\n",
    "pop_gdf.rename(columns={'index':'pop_id'},inplace=True)\n",
    "\n",
    "pop_gdf = pop_gdf[['pop_id','lat','lon','population','geometry','pop_color']].reset_index()\n",
    "pop_gdf = pop_gdf.reset_index()\n",
    "del pop_gdf['index']\n",
    "del pop_gdf['pop_id']\n",
    "pop_gdf.rename(columns={'level_0':'pop_id'},inplace=True) \n",
    "\n",
    "pop_gdf['nearest_node_pop'] = network.get_node_ids(pop_gdf['lon'], \n",
    "                                                    pop_gdf['lat'], mapping_distance=1000)\n",
    "\n",
    "pop_gdf = pd.merge(pop_gdf,df_nodes.reset_index()[['index','x','y']],left_on='nearest_node_pop',\n",
    "                        right_on='index')\n",
    "pop_gdf['road_dist_pop'] = haversine_vectorize(pop_gdf['lon'],pop_gdf['lat'],\n",
    "                                                        pop_gdf['x'],pop_gdf['y'])\n",
    "pop_gdf = pop_gdf[['pop_id','geometry','lat','lon','population','pop_color',\n",
    "                   'nearest_node_pop','road_dist_pop']]\n",
    "\n",
    "pop_gdf = pop_gdf.sort_values('pop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.to_excel('pop.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_sites = gpd.read_file('data/health_sites_who.geojson')\n",
    "health_sites = health_sites[['LAT','LONG','L_NAME','geometry','CLASS']].reset_index()\n",
    "health_sites['lat'] = health_sites['geometry'].apply(lambda x:x.y) \n",
    "health_sites['lon'] = health_sites['geometry'].apply(lambda x:x.x) \n",
    "health_sites.rename(columns={'index':'fac_id','L_NAME':'name'},inplace=True) \n",
    "del health_sites['LAT'] \n",
    "del health_sites['LONG']\n",
    "del health_sites['CLASS']\n",
    "\n",
    "health_sites['nearest_node_fac'] = network.get_node_ids(health_sites['lon'], \n",
    "                                                    health_sites['lat'], mapping_distance=1000)\n",
    "\n",
    "health_sites = pd.merge(health_sites,df_nodes.reset_index()[['index','x','y']],left_on='nearest_node_fac',\n",
    "                        right_on='index')\n",
    "\n",
    "health_sites['road_dist_fac'] = haversine_vectorize(health_sites['lon'],health_sites['lat'],\n",
    "                                                        health_sites['x'],health_sites['y'])\n",
    "health_sites = health_sites[['fac_id','name','geometry','lat','lon','nearest_node_fac','road_dist_fac']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "buffer_hospitals = health_sites[['fac_id','geometry']]\n",
    "buffer_hospitals['geometry'] = health_sites[['fac_id',\n",
    "                                             'geometry']].to_crs('EPSG:32610').buffer(distance_km*1000).to_crs('EPSG:4326').reset_index()[0]\n",
    "\n",
    "points_within_polygon = gpd.sjoin(pop_gdf, buffer_hospitals, op='within')\n",
    "points_within_polygon = points_within_polygon[['fac_id','pop_id']]\n",
    "\n",
    "buffered_pop = points_within_polygon.groupby('fac_id')['pop_id'].apply(list).reset_index()\n",
    "buffered_pop.columns = ['fac_id','pop_with_euclidean_buffer']\n",
    "\n",
    "df_access_fac = pd.merge(health_sites,buffered_pop,on='fac_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_dist = df_access_fac[['fac_id','lat','lon',\n",
    "                             'pop_with_euclidean_buffer',\n",
    "                             'nearest_node_fac','road_dist_fac']].explode('pop_with_euclidean_buffer')\n",
    "\n",
    "travel_dist.rename(columns={'lat':'lat_fac'},inplace=True)\n",
    "travel_dist.rename(columns={'lon':'lon_fac'},inplace=True)\n",
    "\n",
    "travel_dist = pd.merge(travel_dist,pop_gdf[['pop_id','lon','lat','population','nearest_node_pop','road_dist_pop']],\n",
    "         left_on='pop_with_euclidean_buffer',right_on='pop_id')\n",
    "travel_dist.rename(columns={'lat':'lat_pop'},inplace=True)\n",
    "travel_dist.rename(columns={'lon':'lon_pop'},inplace=True)\n",
    "\n",
    "travel_dist['travel_path_km'] = network.shortest_path_lengths(travel_dist['nearest_node_fac'],\n",
    "                                                              travel_dist['nearest_node_pop'])\n",
    "\n",
    "travel_dist['travel_distance'] = travel_dist['road_dist_fac']+travel_dist['road_dist_pop']+travel_dist['travel_path_km']\n",
    "\n",
    "travel_dist['euclidean_distance'] = haversine_vectorize(travel_dist['lon_pop'],travel_dist['lat_pop'],\n",
    "                                                        travel_dist['lon_fac'],travel_dist['lat_fac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_dist['travel_distance_corrected'] = travel_dist[['road_dist_pop','euclidean_distance',\n",
    "                                                           'travel_distance']].apply(get_corrected_distance,\n",
    "                                                                                            axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Added by Joaquim\n",
    "\n",
    "I noticed that line geometries from the road network have been replaced by straight lines. \n",
    "I got curious about the effect.\n",
    "\n",
    "## I learnt this from ChatGPT\n",
    "\n",
    "EPSG:4326, also known as WGS 84 (World Geodetic System 1984), is a geographic coordinate system based on latitude and longitude. It's often used to represent locations on the Earth's surface as spherical coordinates.\n",
    "\n",
    "While EPSG:4326 can be used for mapping and analysis in Timor-Leste, it's not the most suitable coordinate system if you need to perform accurate distance measurements or area calculations, especially over larger distances. Geographic coordinate systems like EPSG:4326 introduce distortion when measuring distances due to the spherical shape of the Earth, which can lead to inaccuracies.\n",
    "\n",
    "Using a projected coordinate system like UTM (EPSG:32752 or EPSG:32753) would provide more accurate measurements for distances and areas, as it minimizes distortion within a specific zone. UTM zones are designed for accurate measurement within their respective zones, making them more appropriate for tasks that involve quantitative analysis or precise measurements.\n",
    "\n",
    "However, if your work primarily involves visual representation, and you're not concerned with high-precision measurements, EPSG:4326 could still be used for mapping purposes in Timor-Leste. It's essential to consider the nature of your project and the level of accuracy required for your analysis when selecting a coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_key(row):\n",
    "    return (row['start_node'].x, row['start_node'].y, row['end_node'].x, row['end_node'].y)\n",
    "\n",
    "idx_road_network = sorted(range(len(road_network)), key=lambda i: sorting_key(road_network.iloc[i]))\n",
    "idx_edges = sorted(range(len(df_edges)), key=lambda i: sorting_key(df_edges.iloc[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame( [road_network.iloc[idx_road_network].geometry.to_crs('EPSG:32752').length.values, ( df_edges.iloc[idx_edges].len_km * 1000 ).values] ).T.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_result = []\n",
    "\n",
    "for each_val in [2,5,10]:\n",
    "    pop_with_access = travel_dist[travel_dist['travel_distance_corrected']<=each_val]\n",
    "    pop_test = list(pop_with_access['pop_id'].unique())\n",
    "    sel_pop = pop_gdf[pop_gdf['pop_id'].isin(pop_test)]\n",
    "    not_sel_pop = pop_gdf[~pop_gdf['pop_id'].isin(pop_test)]\n",
    "    pop_access_perc = ((sel_pop['population'].sum().round()/pop_gdf['population'].sum().round())*100).round(2)\n",
    "    perc_result.append([each_val,pop_access_perc])\n",
    "\n",
    "df_perc_result = pd.DataFrame(perc_result)\n",
    "df_perc_result.columns = ['Distance(km)','% of population with access']\n",
    "df_perc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_facs = travel_dist[['fac_id','pop_id','travel_distance_corrected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_facs.to_pickle('ForOptimization/WP_currentfacs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "potential_locs = gpd.read_file('data/potential_loc_grid1km.geojson')\n",
    "potential_locs = potential_locs.to_crs(\"EPSG:4326\")[['id','geometry']].reset_index()\n",
    "potential_locs = potential_locs[['index','geometry']]\n",
    "potential_locs['Latitude'] = potential_locs['geometry'].apply(lambda x:x.y)\n",
    "potential_locs['Longitude'] = potential_locs['geometry'].apply(lambda x:x.x)\n",
    "potential_locs.rename(columns={'index':'fac_id'},inplace=True)\n",
    "\n",
    "potential_locs['nearest_node_fac'] = network.get_node_ids(potential_locs['Longitude'], \n",
    "                                                    potential_locs['Latitude'], mapping_distance=1000)\n",
    "\n",
    "potential_locs = pd.merge(potential_locs,df_nodes.reset_index()[['index','x','y']],left_on='nearest_node_fac',\n",
    "                        right_on='index')\n",
    "\n",
    "potential_locs['road_dist_fac'] = haversine_vectorize(potential_locs['Longitude'],potential_locs['Latitude'],\n",
    "                                                        potential_locs['x'],potential_locs['y'])\n",
    "\n",
    "potential_locs = potential_locs[['fac_id','geometry','Latitude','Longitude','nearest_node_fac','road_dist_fac']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "buffer_hospitals = potential_locs[['fac_id','geometry']]\n",
    "buffer_hospitals['geometry'] = potential_locs[['fac_id',\n",
    "                                             'geometry']].to_crs('EPSG:32610').buffer(distance_km*1000).to_crs('EPSG:4326').reset_index()[0]\n",
    "\n",
    "points_within_polygon = gpd.sjoin(pop_gdf, buffer_hospitals, op='within')\n",
    "points_within_polygon = points_within_polygon[['fac_id','pop_id']]\n",
    "\n",
    "buffered_pop = points_within_polygon.groupby('fac_id')['pop_id'].apply(list).reset_index()\n",
    "buffered_pop.columns = ['fac_id','pop_with_euclidean_buffer']\n",
    "\n",
    "df_access_fac = pd.merge(potential_locs,buffered_pop,on='fac_id')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_dist = df_access_fac[['fac_id','Latitude','Longitude',\n",
    "                             'pop_with_euclidean_buffer',\n",
    "                             'nearest_node_fac','road_dist_fac']].explode('pop_with_euclidean_buffer')\n",
    "\n",
    "travel_dist.rename(columns={'Latitude':'lat_fac'},inplace=True)\n",
    "travel_dist.rename(columns={'Longitude':'lon_fac'},inplace=True)\n",
    "\n",
    "travel_dist = pd.merge(travel_dist,pop_gdf[['pop_id','lon','lat',\n",
    "                                            'population','nearest_node_pop','road_dist_pop']],\n",
    "                       \n",
    "         left_on='pop_with_euclidean_buffer',right_on='pop_id')\n",
    "travel_dist.rename(columns={'lat':'lat_pop'},inplace=True)\n",
    "travel_dist.rename(columns={'lon':'lon_pop'},inplace=True)\n",
    "\n",
    "travel_dist['travel_path_km'] = network.shortest_path_lengths(travel_dist['nearest_node_fac'],\n",
    "                                                              travel_dist['nearest_node_pop'])\n",
    "\n",
    "travel_dist['travel_distance'] = travel_dist['road_dist_fac']+travel_dist['road_dist_pop']+travel_dist['travel_path_km']\n",
    "\n",
    "travel_dist['euclidean_distance'] = haversine_vectorize(travel_dist['lon_pop'],travel_dist['lat_pop'],\n",
    "                                                        travel_dist['lon_fac'],travel_dist['lat_fac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_dist['travel_distance_corrected'] = travel_dist[['road_dist_pop','euclidean_distance',\n",
    "                                                           'travel_distance']].apply(get_corrected_distance,\n",
    "                                                                                            axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_result = []\n",
    "\n",
    "for each_val in [2,5,10]:\n",
    "    pop_with_access = travel_dist[travel_dist['travel_distance_corrected']<=each_val]\n",
    "    pop_test = list(pop_with_access['pop_id'].unique())\n",
    "    sel_pop = pop_gdf[pop_gdf['pop_id'].isin(pop_test)]\n",
    "    not_sel_pop = pop_gdf[~pop_gdf['pop_id'].isin(pop_test)]\n",
    "    pop_access_perc = ((sel_pop['population'].sum().round()/pop_gdf['population'].sum().round())*100).round(2)\n",
    "    perc_result.append([each_val,pop_access_perc])\n",
    "\n",
    "df_perc_result = pd.DataFrame(perc_result)\n",
    "df_perc_result.columns = ['Distance(km)','% of population with access']\n",
    "df_perc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_facs = travel_dist[['fac_id','pop_id','travel_distance_corrected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_facs.to_pickle('ForOptimization/WP_potentialfacs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_gdf.to_pickle('ForOptimization/WP_population.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_facs = current_facs.round()\n",
    "potential_facs = potential_facs.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_facs['fac_id'] = potential_facs['fac_id']+current_facs['fac_id'].max()+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([current_facs, potential_facs],axis=0)\n",
    "df_combined.columns = ['fac_id','pop_id','distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_gdf.sort_values(by='pop_id')['population'].values.round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "budget = [5,10,20,30,40,50,60,70,80,90,100,150,200,250,300,400,500,750,1000,2000]\n",
    "\n",
    "array_household = pop_gdf.sort_values(by='pop_id')['population'].values.round().astype(int)\n",
    "current_hospitals_ID = current_facs['fac_id'].unique()\n",
    "new_hospitals_ID = potential_facs['fac_id'].unique()\n",
    "    \n",
    "df_combined_output = pd.DataFrame()\n",
    "for each_threshold in [2,5,10]:\n",
    "    opt_array, tModelling, tSolving = OptimizationModel(array_household, current_hospitals_ID, \n",
    "                                                        new_hospitals_ID, df_combined, each_threshold, \n",
    "                                                        budget)\n",
    "    \n",
    "    df_opt_outputs = pd.DataFrame(opt_array)\n",
    "    \n",
    "    df_combined_output = pd.concat([df_combined_output,df_opt_outputs])\n",
    "    print(\"Threshold distance: \" + str(each_threshold))\n",
    "    print(\"Solving time: \" + str(tSolving/60) + \", modelling time: \" + str(tModelling/60))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_output['%'] = df_combined_output['%'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_combined_output.sort_values(by=['km','number_of_hospitals']),\n",
    "              x='number_of_hospitals',y='%',color='km',\n",
    "              labels={\n",
    "                     \"number_of_hospitals\": \"Number of Health Facilities\",\n",
    "                     \"%\": \"Percentage of households with access\",\n",
    "                     \"km\": \"Distance (KM)\"\n",
    "                 })\n",
    "fig.update_xaxes(range=[0, 2500])\n",
    "fig.update_yaxes(range=[0, 110])\n",
    "\n",
    "fig.add_annotation(x=165, y=105,\n",
    "                   text=\"Current health facilities:\"+str(current_facs['fac_id'].nunique()),\n",
    "                   showarrow=False,\n",
    "                   arrowhead=1)\n",
    "\n",
    "fig.add_shape(type=\"line\",\n",
    "              x0=current_facs['fac_id'].nunique(), y0=0, x1=current_facs['fac_id'].nunique(), y1=120,\n",
    "              line=dict(color=\"RoyalBlue\",width=1)\n",
    "             )\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getindices = lambda x : np.where(np.round(x))[0]\n",
    "df_combined_output['sol'] = df_combined_output.array_hosp.apply( getindices )\n",
    "df_combined_output['served'] = df_combined_output.array_hh.apply( getindices )\n",
    "df_combined_output['coverage_from_served'] = df_combined_output.served.apply( lambda x : w[x].sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = { km : ExtractOptimizationDataFromTravelDistanceMatrix( df_combined, km ) for km in set(df_combined_output.km) }\n",
    "\n",
    "find_served = lambda sol, indexed : mc.all_in( [ indexed[s] for s in set(sol) & set(indexed.keys()) ] )\n",
    "\n",
    "df_combined_output['served_from_sol'] = [ find_served(sol,aux[km]) for km,sol in df_combined_output[['km','sol']].values ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "( df_combined_output.served_from_sol.apply(set) == df_combined_output.served.apply(set) ).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold = 2\n",
    "tot_pop = sorted(df_combined.pop_id.unique())\n",
    "tot_fac = sorted(df_combined.fac_id.unique())\n",
    "set(np.diff(tot_pop)),set(np.diff(tot_fac)),set(tot_pop) - set( df_combined[df_combined.distance <= 2].pop_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = ExtractOptimizationDataFromTravelDistanceMatrix( df_combined, distance_threshold )\n",
    "assert set( df_combined[df_combined.distance <= 2].pop_id ) == set( mc.all_in( list(indexed.values()) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = array_household\n",
    "I, J, IJ, JI = mc.CreateIndexMapping( indexed, w )\n",
    "sum(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(I),len(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_opt = mc.OptimizeWithGurobipy( w, I, J, IJ, already_open=list(current_hospitals_ID), budget_list=[b + len(current_hospitals_ID) for b in budget], mipGap=.001, progress=tqdm, parsimonious=False, maxTimeInSeconds=60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_opt['pc'] = aux_opt['value'] / sum(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_opt['served_from_sol'] = aux_opt.solution.apply( lambda sol : find_served(sol,indexed) )\n",
    "aux_opt['coverage_from_served'] = aux_opt.served_from_sol.apply( lambda served : w[served].sum() )\n",
    "aux_opt['pc_from_coverage'] = aux_opt['coverage_from_served'] / w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "( aux_opt.value - aux_opt.coverage_from_served ).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diff( aux_opt.pc_from_coverage ).min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
